---
title: 'Lab 3'
author: "Alina Bodnar, Anastasiia Yosypiv, Ivan Maksymchuk"
output:
  html_document:
    df_print: paged
editor_options: 
  markdown: 
    wrap: sentence
---

## Work breakdown:

-   Alina Bodnar - Problem 1

-   Anastasiia Yosypiv - Problem 2

-   Ivan Maksymchuk - Problem 3

## Part 1

### Problem 1

#### Problem formulation

In this problem we work with i.i.d. samples X1,â€¦,Xn from an exponential distribution Exp(Î»), whose mean is E[X]=1/Î»=Î¸.
For this task, the true parameter is set to Î¸=id_num/10=1.7.
The sample mean XË‰ is a natural estimate of Î¸.

The goal of the task is to compare four different ways of constructing a confidence interval for Î¸, and to check using simulations whether these intervals actually contain the true parameter with probability close to the chosen confidence level 1âˆ’Î±.
To study this properly, we vary the sample size nâˆˆ{5,10,30,100}, the confidence level Î±âˆˆ{0.1,0.05,0.01}, and the number of repetitions mâˆˆ{100,1000,5000}.
This allows us to examine how well each method performs in terms of empirical coverage and how long the resulting intervals are.

```{r}
id_num <- 17
set.seed(id_num)
theta_true <- id_num / 10
lambda_true <- 1 / theta_true
alphas <- c(0.1, 0.05, 0.01)
n_vals <- c(5, 10, 30, 100)
m_vals <- c(100, 1000, 5000)  
```

```{r}
# 1: exact chi-square-based confidence interval
ci1_theta <- function(x, alpha) {
  n <- length(x)
  xbar <- mean(x)
  chi_low <- qchisq(alpha / 2, df = 2 * n)       # Ï‡Â²_{Î±/2, 2n}
  chi_up  <- qchisq(1 - alpha / 2, df = 2 * n)   # Ï‡Â²_{1-Î±/2, 2n}
  lower <- 2 * n * xbar / chi_up
  upper <- 2 * n * xbar / chi_low
  c(lower, upper)
}

```

```{r}
# 2: normal approximation with known variance (uses true Î¸)
ci2_theta <- function(x, alpha, theta_true) {
  n <- length(x)
  xbar <- mean(x)
  z <- qnorm(1 - alpha / 2)
  half_len <- z * theta_true / sqrt(n)
  c(xbar - half_len, xbar + half_len)
}
```

```{r}
# 3: rearranged inequality (no Î¸ inside)
ci3_theta <- function(x, alpha) {
  n <- length(x)
  xbar <- mean(x)
  z <- qnorm(1 - alpha / 2)
  c <- z / sqrt(n)
  
  if (c >= 1) {
    return(c(NA, NA))
  }
  
  lower <- xbar / (1 + c)
  upper <- xbar / (1 - c)
  c(lower, upper)
}
```

```{r}
# 4: t-approximation with sample standard deviation
ci4_theta <- function(x, alpha) {
  n <- length(x)
  xbar <- mean(x)
  s <- sd(x)
  tcritical <- qt(1 - alpha / 2, df = n - 1)
  half_len <- tcritical * s / sqrt(n)
  c(xbar - half_len, xbar + half_len)
}
```

```{r}
#simulation for one scenario
simulate_one <- function(n, alpha, m_reps) {
  cover <- numeric(4) #cover[i] counts how many times method contains the true Î¸
  lengths <- numeric(4) #lengths[i] adds up all the interval lengths from a method
  
  for (m in 1:m_reps) {
    x <- rexp(n, rate = lambda_true)
    
    # method 1
    ci1 <- ci1_theta(x, alpha)
    cover[1]  <- cover[1]  + (ci1[1] <= theta_true && theta_true <= ci1[2])
    lengths[1] <- lengths[1] + diff(ci1)
    
    # method 2
    ci2 <- ci2_theta(x, alpha, theta_true)
    cover[2]  <- cover[2]  + (ci2[1] <= theta_true && theta_true <= ci2[2])
    lengths[2] <- lengths[2] + diff(ci2)
    
    # method 3
    ci3 <- ci3_theta(x, alpha)
    if (!any(is.na(ci3))) {
      cover[3]  <- cover[3]  + (ci3[1] <= theta_true && theta_true <= ci3[2])
      lengths[3] <- lengths[3] + diff(ci3)
    }
    
    # method 4
    ci4 <- ci4_theta(x, alpha)
    cover[4]  <- cover[4]  + (ci4[1] <= theta_true && theta_true <= ci4[2])
    lengths[4] <- lengths[4] + diff(ci4)
  }
  
  data.frame(
    n = n,
    alpha = alpha,
    m_reps = m_reps,
    Method = 1:4,
    Coverage = cover / m_reps,
    AvgLength = lengths / m_reps
  )
}
```

```{r}
#runs the simulation for all (n, Î±, m)
results <- data.frame()

for (alpha in alphas) {
  for (n in n_vals) {
    for (m_reps in m_vals) {
      res <- simulate_one(n, alpha, m_reps)
      results <- rbind(results, res)
    }
  }
}
results
```

### Analysis

**Coverage Accuracy**

The goal of the simulation was to check whether each confidence interval method contains the true parameter Î¸=1.7 with probability approximately equal to the nominal level 1âˆ’Î±

Across all sample sizes and confidence levels, the results show:

Method 1â€” Highly accurate

-   Coverage is consistently very close to the target value.\
    For example:

    -   Î± = 0.10: coverage â‰ˆ 0.90â€“0.93

    -   Î± = 0.05: coverage â‰ˆ 0.94â€“0.97

    -   Î± = 0.01: coverage â‰ˆ 0.98â€“1.00

        This holds even when **n = 5**, confirming that Method 1â€™s exactness does not depend on asymptotics.

**Method 2** â€” Acceptable, improves with n

-   Overcoverage appears for small n (n=5, Î±=0.10 â†’ 0.92â€“0.93 rather than 0.90).

-   But for n â‰¥ 30 the coverage is close to nominal.

**Method 3 (Rearranged inequality, corrected method)** â€” Behaves exactly like Method 2 in coverage

-   Method 3 coverage matches Method 2 almost identically, which is expected because both derive from the same normal approximation.

Examples:

-   n = 10, Î±=0.10 â†’ 0.9086 (Method 2) vs 0.9086 (Method 3)

-   n = 30, Î±=0.05 â†’ 0.9466 (Methods 2 & 3)

-   n = 100, Î±=0.05 â†’ 0.9516 (both methods)

The one place where Method 3 fails is when nâ‰¤z1âˆ’Î±/2

This happens only when (n=5, Î±=0.01), and Method 3 produced no intervals at all, giving coverage = 0.
In this case the CI simply cannot exist.

**Method 4** â€” Slight undercoverage for small n, accurate for larger n

-   For n=5: coverage ranges 0.83â€“0.89 (slightly below the target).

-   For n=10: coverage improves to \~0.90â€“0.95.

-   For n â‰¥ 30: coverage stabilizes and closely matches the nominal level.

This matches the theory that the t-approximation only becomes accurate when sample size is moderate.

```{r}
plot_ci_lengths <- function(n, alpha) {
  m_reps <- m_vals[length(m_vals)]
  lengths <- matrix(NA, nrow = m_reps, ncol = 4)
  
  for (m in 1:m_reps) {
    x <- rexp(n, rate = lambda_true)
    
    lengths[m,1] <- diff(ci1_theta(x, alpha))
    lengths[m,2] <- diff(ci2_theta(x, alpha, theta_true))
    
    tmp <- ci3_theta(x, alpha)
    lengths[m,3] <- if (!any(is.na(tmp))) diff(tmp) else NA
    
    lengths[m,4] <- diff(ci4_theta(x, alpha))
  }
  
  par(mfrow = c(2,2), oma = c(0, 0, 3, 0))

  for (i in 1:4) {
    hist(lengths[,i],
         main = paste("Method", i),
         xlab = "Interval length",
         col = "lightgray")
  }
  
  title_text <- paste("Histogram of CI Lengths  |  n =", n, ", alpha =", alpha)
  mtext(title_text, outer = TRUE, cex = 1.4, font = 2)
}


plot_ci_lengths(10, 0.05)
plot_ci_lengths(30, 0.05)
plot_ci_lengths(100, 0.05)

plot_ci_lengths(30, 0.1)
plot_ci_lengths(30, 0.01)

```

## Analysis

As the sample size increases, all four confidence interval methods become shorter and more stable.
But their behavior differs significantly, especially for small samples.

#### Method 1:

It relies on the exact distribution of 2Î»nXË‰, which follows a chi-square distribution with 2n degrees of freedom.
For Method 1 we use the fact that if X_i \~ Exp(Î»), then sum(X_i) has a Gamma(n, Î») distribution, and therefore 2 Î» sum(X_i) \~ Ï‡Â²\_{2n}.
Since mean(X) = sum(X_i)/n, this yields an exact confidence interval for Î¸ = 1/Î».

-   For every n, Method 1 produces moderately short intervals.

-   The interval lengths are fairly stable and bell-shaped.

-   For small samples (n=10), lengths are around 2â€“4.

-   For n=100, they shrink to about 0.5â€“0.9.

#### Method 2:

It uses the normal approximation of XË‰, assuming that the variance Î¸\^2 is known.

-   Always produces a constant interval length (histogram is a single bar).

-   The length decreases with larger n, but it does not adapt to sample variation.

-   For small n the approximation is unreliable, often resulting in coverage slightly above the target level.

This method is not reliable unless n is large.

#### Method 3:

It tries to solve the inequality from Method 2 for Î¸, so that the resulting interval no longer depends on the unknown parameter.
This interval only exists if $1 - z / \sqrt{n} > 0$ = $$
\sqrt{n} > z_{1 - \alpha/2}
$$

-   The intervals from Method 3 are usually a bit longer than those from Method 2, since the formula involves dividing by $1 \pm z / \sqrt{n}$ . Still, for moderate sample sizes the intervals behave normally.
-   The only time Method 3 performs poorly is when n is very small and Î± is very small. In this case, the interval does not exist at all (for example, n=5, Î±=0.01).

#### Method 4:

Method 4 replaces the unknown variance with the sample standard deviation and uses a t-critical value.
For small n, this introduces noticeable variability and undercoverage (around 0.83â€“0.89 for n=5).
However, the method stabilizes quickly, and by n=30 its performance becomes close to the nominal level.Interval lengths are slightly wider than Method 2 or 3 for small n but become comparable for moderate sample sizes.

## Which confidence interval method is best?

Among the four confidence interval methods, the exact chi-square interval from the 1st method is the most reliable, based on both coverage and interval length.
The reason is that this method is derived directly from the exact distribution of the statistic 2Î»nð‘‹Ë‰, which follows a chi-square distribution with 2n degrees of freedom for exponential data.
Because this result is exact and does not rely on any approximations, the method produces intervals whose coverage probability is extremely close to the intended level, even when the sample size is very small.
In addition, the interval lengths are fairly stable from sample to sample and not excessively wide.
This makes Method 1 both accurate and efficient.

Method 4 works well once the sample size is moderately large.

In our simulations:

-   For n=5, coverage is slightly too low (around 0.83â€“0.89 depending on Î±).

-   For n = 10, coverage improves but is still slightly below the target.

-   By n=30, coverage is very close to the nominal level.

-   For n=100, Method 4 performs almost as well as the exact method.

    This happens because the sample variance becomes much more stable once n is around 30 or higher.

    So, Method 4 is a reasonable practical choice when nâ‰¥30.

Both Method 2 and Method 3 come from the same normal approximation to the distribution of XË‰.\
They behave almost identically in the simulations:

-   coverage is correct when n is moderate or large,

-   coverage tends to be slightly above the target level when n is very small,

-   interval lengths shrink correctly as n increases, Method 3 produces no interval when n is too small relative to Î±

    Because Method 2 uses the true value of the parameter in the standard error, it is not realistic outside of simulations.
    Method 3 removes Î¸ from the formula, but it gives nearly the same coverage as Method 2 and produces slightly longer intervals.


## Problem 2

Poisson distribution: $X_i \sim Pois(\theta)$

Part 2: Normal approximation using true theta in SE.

We approximate the sample mean $X$ using a normal distribution:

$$
X \sim N(\mu, \sigma^2)
$$

where:

-   $\mu = \theta$
-   $\sigma^2 = \frac{s^2}{n}$
-   $s^2 = \theta^2$ (population variance of $\text{Exp}(\lambda)$)

We define the standardized statistic:

$$
Z := \frac{\sqrt{n}(X - \theta)}{\theta}
$$

Since $Z \approx N(0,1)$, we get:

$$
P(|\theta - X| \le z_{\beta} \frac{\theta}{\sqrt{n}}) = P(|Z| \le z_{\beta}) = 2\beta - 1.
$$

So we conclude that with probability $2\beta - 1$, the true parameter $\theta$ lies within: $$X \pm z_{\beta}\frac{\theta}{\sqrt{n}}.
> $$

```{r}
ci2_poisson <- function(x, alpha, theta_true) {
  n <- length(x)
  xbar <- mean(x)
  z <- qnorm(1 - alpha/2)
  half_len <- z * sqrt(theta_true / n)
  c(xbar - half_len, xbar + half_len)
}
```

Part 3: Rearranged inequality**.** Solving the Inequality for $\theta$.

Since the interval above still depends on the unknown $\theta$, we solve the inequality:

$$
|\theta - X| \le z_{\beta}\frac{\theta}{\sqrt{n}}
$$

for $\theta$.

This yields a new confidence interval with the same confidence level $2\beta - 1$, but importantly:

This rewritten interval **does not depend on the unknown parameter** $\theta$.

```{r}
ci3_poisson <- function(x, alpha) {
  n <- length(x)
  xbar <- mean(x)
  z <- qnorm(1 - alpha/2)
  delta <- z / sqrt(n)
  B <- 2 * xbar + delta^2
  disc <- B^2 - 4 * xbar^2
  if (disc < 0) return(c(NA, NA))
  sqrt_disc <- sqrt(disc)
  lower <- (B - sqrt_disc) / 2
  upper <- (B + sqrt_disc) / 2
  lower <- max(lower, 0)
  c(lower, upper)
}
```

Part 4: Using Student's t-Distribution.

A more general and practical way to eliminate the dependence on $\theta$ is to estimate the population standard deviation using the **sample standard deviation**.

Then, instead of using the normal approximation, we use the **Student t-distribution**.

```{r}
ci4_poisson <- function(x, alpha) {
  n <- length(x)
  xbar <- mean(x)
  s <- sd(x)
  tcritical <- qt(1 - alpha/2, df = n - 1)
  half_len <- tcritical * s / sqrt(n)
  c(xbar - half_len, xbar + half_len)
}
```

Simulation:

```{r}
simulate_one <- function(n, alpha, m_reps) {
  cover <- numeric(3)
  lengths <- numeric(3)
  
  for (m in 1:m_reps) {
    x <- rpois(n, lambda = theta_true)
    
    # method 2
    ci2 <- ci2_poisson(x, alpha, theta_true)
    cover[1]  <- cover[1]  + (ci2[1] <= theta_true && theta_true <= ci2[2])
    lengths[1] <- lengths[1] + diff(ci2)
    
    # method 3
    ci3 <- ci3_poisson(x, alpha)
    if (!any(is.na(ci3))) {
      cover[2]  <- cover[2]  + (ci3[1] <= theta_true && theta_true <= ci3[2])
      lengths[2] <- lengths[2] + diff(ci3)
    } else {
      # If CI doesn't exist (rare for these params), count it as NOT covering and skip length.
      # (This matches pragmatic behavior: no interval -> we don't add length, coverage not incremented.)
    }
    
    # method 4
    ci4 <- ci4_poisson(x, alpha)
    cover[3]  <- cover[3]  + (ci4[1] <= theta_true && theta_true <= ci4[2])
    lengths[3] <- lengths[3] + diff(ci4)
  }
  
  data.frame(
    n = n,
    alpha = alpha,
    m_reps = m_reps,
    Method = c("Normal_trueVar","Rearranged","T_sampleSD"),
    Coverage = cover / m_reps,
    AvgLength = lengths / m_reps,
    stringsAsFactors = FALSE
  )
}


results_poisson <- data.frame()
for (alpha in alphas) {
  for (n in n_vals) {
    for (m_reps in m_vals) {
      res <- simulate_one(n, alpha, m_reps)
      results_poisson <- rbind(results_poisson, res)
    }
  }
}


results_poisson$Coverage <- round(results_poisson$Coverage, 4)
results_poisson$AvgLength <- round(results_poisson$AvgLength, 4)


print(results_poisson)
```

#### (a) Coverage Probability

The aim of this part was to check whether the constructed confidence intervals of level $1 - \alpha$ contain the true parameter $\theta = \frac{1}{\lambda}$ approximately $100(1 - \alpha)\%$ of the time. Based on the simulation results, the following patterns are observed:

-   When the sample size is very small (like $n = 5$):
    -   The **Normal** and **Rearranged** confidence intervals tend to slightly *overcover*, meaning their empirical coverage is above the nominal level.
    -   The **t-based confidence interval** shows noticeably lower coverage (â‰ˆ 85â€“88%), indicating that it misses the true parameter more often than expected.
-   For moderate sample sizes **(**$n = 10$ to $n = 30$):
    -   All methods begin approaching the nominal confidence level.
    -   The gap between them becomes smaller.
-   For larger sample sizes **(**$n \ge 100$):
    -   All three methods stabilize and achieve coverage close to the target level.
    -   Differences become minimal and are mostly due to simulation variation.

Overall, the empirical coverage improves as the sample size increases, and the Normal-based methods perform more reliably for smaller samples.

#### (b) Comparison of Precision

To evaluate precision, the average lengths of the confidence intervals were compared. The results indicate:

-   For very small samples, the ranking is:

    1.  **Normal CI** â†’ shortest (best precision)
    2.  **Rearranged CI** â†’ slightly wider
    3.  **t-based CI** â†’ widest (least precise)

-   As the sample size increases, all interval types become shorter.

-   For $n \ge 30$, the differences in interval length between methods become much smaller.

In summary, the Normal method provides the narrowest intervals and therefore the best precision, especially when the sample size is small.

#### (c) Recommendation

Based on both coverage probability and interval length:

-   The **Normal and Rearranged confidence intervals** show good performance overall and remain reliable even with small sample sizes.
-   The **t-based confidence interval** tends to underperform for small samples due to wider intervals and lower empirical coverage. However, it becomes competitive when the sample size is sufficiently large.

**Final Recommendation:**

-   For small samples ($n \le 10$), the **Normal-based methods** (Normal and Rearranged) are preferable.
-   For **moderate or large samples (**$n \ge 30$), all three methods perform adequately, but the Normal method may be preferred due to slightly shorter interval lengths.


# Part 2, Problem 3

```{r}
set.seed(team_id)

mu <- 10
sigma_squared <- 4
sigma <- sqrt(sigma_squared)
M <- 10000 # Number of repetitions to estimate Expectation
sample_sizes <- c(10, 50, 100, 1000)

results <- data.frame(
  n = integer(),
  Bias_n = double(),
  Bias_n_minus_1 = double(),
  Mean_Est_n = double(),
  Mean_Est_n_minus_1 = double()
)

for (n in sample_sizes) {
  # Generate M samples of size n
  # Creates a matrix where each column is a sample of size n
  samples <- matrix(rnorm(n * M, mean = mu, sd = sigma), nrow = n)
  
  means <- colMeans(samples)
  
  # Calculate sum of squared deviations for each column (S_xx)
  # sweep subtracts the column means from the matrix
  deviations <- sweep(samples, 2, means, "-")
  S_xx <- colSums(deviations^2)
  
  est_n <- S_xx / n           # Biased estimator
  est_n_1 <- S_xx / (n - 1)   # Unbiased estimator
  
  # Calculate Empirical Bias
  bias_n <- mean(est_n) - sigma_squared
  bias_n_1 <- mean(est_n_1) - sigma_squared
  
  results <- rbind(results, data.frame(
    n = n,
    Bias_n = bias_n,
    Bias_n_minus_1 = bias_n_1,
    Mean_Est_n = mean(est_n),
    Mean_Est_n_minus_1 = mean(est_n_1)
  ))
}

print(results)

# Visualization of Bias Convergence
barplot(
  rbind(results$Bias_n, results$Bias_n_minus_1), 
  beside = TRUE, 
  names.arg = results$n, 
  col = c("red", "green"), 
  legend.text = c("Biased (1/n)", "Unbiased (1/n-1)"),
  main = "Bias of Variance Estimators vs Sample Size",
  xlab = "Sample Size (n)",
  ylab = "Bias"
)
```
## task d)
For small n (like 10):
  The bias of $ \sigma_{n}^2 $ is significant and negative (approximately âˆ’0.4 for $\sigma^{2}=4$). This aligns with the theoretical bias $ -\frac{\sigma^2}{n} = -0.4$
  The bias of $ \sigma_{n-1}^{2} $ is extremely close to 0

For large n (like 1000):
  The bias of $ \sigma_{n}^2 $ shrinks towards 0 (approx âˆ’0.004).
  Both estimators yield values very close to the population variance.

The estimator $ \sigma_{n-1}^{2} $ consistently provides an estimate centered around the true parameter $\sigma^{2}=4$ regardless of sample size. The estimator $ \sigma_{n}^2 $ systematically underestimates the variance. This underestimation is most severe at small sample sizes but becomes negligible as n increases.

## task e-f
First we need to calculate the the sum of squared deviations $ S_{xx} = \sum_{i=1}^{n}{(X_i-\overline{X})^2}$
We know that $\sum(X_i - \overline{X})^2 = \sum{X_i^2} - n \overline{X}^2$, and given properties of expectation and variance we get:
$$
1. E(X_i^2) = \sigma^2 + \mu^2
$$
$$
2. E(X_i^2) = Var(\overline{X} + (E(\overline{X}))^2) = \frac{\sigma^2}{n} + \mu^2
$$
So, substituting it into expectation of the $S_{xx}$ we get:
$$
E(S_{xx}) = \sum_{i=1}^{n}{E(X_i^2)} - nE(\overline{X}^2) = n(\sigma^2+\mu^2) - n(\frac{\sigma^2}{n} + \mu^2) = (n-1)\sigma^2
$$
So, we can substitute it into formula of $E(\sigma_n^2)$ and $E(\sigma_{n-1}^2)$ and we will get that $E(\sigma_n^2) = \sigma^2 - \frac{\sigma^2}{n}$ and $E(\sigma_{n-1}^2) = \sigma^2$.

So $\sigma_{n-1}^2)$ is unbiased estimator as $E(\hat\theta) = \theta$ and $\sigma_n^2$ is a biased estimator as the equation mentioned previously doesn't hold for it.

## task g
The theoretical derivation confirms that using n in the denominator leads to a biased estimate because it does not account for the fact that the sample mean $\overline{X}$ minimizes the sum of squared deviations for that specific sample, making the spread appear smaller than it is relative to the true population mean $\mu$. Also it is important to use $\sigma_{n-1}^2$ estimator for small datasets as it doesnot give any error, but for large dataset it doesn't matter wether you use $\sigma_{n-1}^2$ or $\sigma_{n}^2$ as $E(\sigma_{n}^2) \rightarrow \sigma^2$ as $n \rightarrow \infty$, meaning it it asymptotically unbiased.
